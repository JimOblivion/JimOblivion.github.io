{"meta":{"title":"Oblivion","subtitle":"","description":"","author":"Jim Oblivion","url":"http://jimoblivion.github.io","root":"/"},"pages":[{"title":"文章归档","date":"2023-07-12T06:02:30.429Z","updated":"2023-07-12T06:02:30.429Z","comments":true,"path":"archive.html","permalink":"http://jimoblivion.github.io/archive.html","excerpt":"","text":""},{"title":"about","date":"2023-07-11T15:04:50.000Z","updated":"2023-07-11T15:04:50.995Z","comments":true,"path":"about/index.html","permalink":"http://jimoblivion.github.io/about/index.html","excerpt":"","text":""}],"posts":[{"title":"Kubernetes实战笔记（十八）：污点、容忍和亲和力","slug":"Kubernetes实战笔记（十八）：污点、容忍和亲和力","date":"2023-07-21T09:10:16.000Z","updated":"2023-07-21T10:49:20.670Z","comments":true,"path":"2023/07/21/Kubernetes实战笔记（十八）：污点、容忍和亲和力/","link":"","permalink":"http://jimoblivion.github.io/2023/07/21/Kubernetes%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0%EF%BC%88%E5%8D%81%E5%85%AB%EF%BC%89%EF%BC%9A%E6%B1%A1%E7%82%B9%E3%80%81%E5%AE%B9%E5%BF%8D%E5%92%8C%E4%BA%B2%E5%92%8C%E5%8A%9B/","excerpt":"","text":"污点和容忍度基于污点的驱逐: 通过污点和容忍度，可以灵活地让pod避开某些节点或者将pod从某些节点驱逐。 如果Pod不能忍受这类污点，Pod会马上被驱逐 如果Pod能忍受这类污点，但是容忍度中没有指定tolerationSeconds， 则Pod还会一直在这个节点上运行 如果Pod能忍受这类污点，而且指定了tolerationSeconds, 则Pod还能在这个节点上继续运行到指定的时间长度。 当某种条件为真时，节点控制器会自动给节点添加污点。当前内置污点包括： Not-ready: 节点未准备好 Unreachable: 节点控制器访问不到节点 Memory-pressure: 节点存在内存压力 Disk-pressure: 节点存在磁盘压力 Pid-pressure: 节点的PID压力 Network-pressure: 节点网络不可用 Unschedulable: 节点不可调度 Uninitialized: 如果kubelet启动时指定了一个云平台驱动，它将给当前节点添加一个污点标志位不可用。在cloud-controller-manager的一个控制器初始化这个节点后，kubelet将删除这个污点。 taint污点 标注在节点上，当我们在一个节点上打上污点以后，k8s会任务尽量不要将pod调度到该节点上，排除该pod上面便是可以容忍该污点，且一个节点可以打多个污点，此时则需要pod容忍所有污点该节点才会被调度。 污点的影响： NoSchedule: 不能容忍的pod不能被调度到该节点，但是已经存在的节点不会被驱逐 NoExecute: 不能容忍的节点会被立即清除，能容忍且没有配置 tolerationSeconds: 未设置，则可以一直运行；设置了3600属性，则该pod还能继续在该节点继续运行3600秒 toleration容忍 标注在Pod上，当pod被调度时，如果没有配置配置容忍，则该pod不会被调度到有污点的节点上，只有该pod上标注了满足某个节点的所有污点，则会被调度到这些节点。 比较操作类型 Equal: 则意味着必须与污点值做匹配，key&#x2F;value都必须相同，才表示能够容忍该污点 Exists: 容忍与污点的比较key，不比较value， 不关系value是什么，只要key存在，就表示可以容忍 污点 给节点打上污点 1kubectl taint no k8snode1 memory=low:NoSchedule Pod正常运行， 此时原本已经运行的Pod不受影响 1kubectl -n zy-test get po -o wide 对Node进行描述 1kubectl -n zy-test describe no k8snode1 删除Pod 1kubectl -n zy-test delete po nginx-deploy-787859bfbd-j2v5p 查看节点Pod，因为没有可用节点所以新Pod会一直处于Pending状态 1kubectl -n zy-test get po -o wide 删除污点 1kubectl taint no k8snode1 memory=low:NoSchedule- 查看节点容器状态，pod已经恢复在k8snode1节点上运行 1kubectl -n zy-test get po -o wide 容忍 编辑deploy, 让pod可以容忍memory&#x3D;low的标签。 1kubectl -n zy-test edit deploy nginx-deploy 1234567891011template: metadata: creationTimestamp: null labels: app: nginx-deploy spec: tolerations: - key: &quot;memory&quot; operator: &quot;Equal&quot; value: &quot;low&quot; effect: &quot;NoSchedule&quot; 给节点打上标签 1kubectl taint no k8snode1 memory=low:NoSchedule 查看最新pod并且删除 删除pod 1kubectl -n zy-test delete po nginx-deploy-75b764c64b-xdtd4 查看pod又在当前节点创建了新的pod 1kubectl -n zy-test get po -o wide 修改operator为Exits，和Equal同理，节点Pod还是可以正常创建 1234tolerations:- effect: NoSchedule key: memory operator: Exists 修改 去除污点 1kubectl taint no k8snode1 memory=low:NoSchedule- 修改 1kubectl -n zy-test edit deploy nginx-deploy 1234tolerations:- effect: NoExecute key: memory operator: Exists 新增污点 1kubectl taint no k8snode1 memory=low:NoExecute 查看容器状态，因为nginx-deploy添加了容忍所以可以存活，其他pod则被驱逐 1kubectl -n zy-test get po -o wide 修改nginx-deploy新增参数 1kubectl -n zy-test edit deploy nginx-deploy 12345tolerations:- effect: NoExecute key: memory operator: Exists tolerationSeconds: 15 等待15秒后，查看Pod的运行状态，出现Pod频繁被销毁重新创建的情况 1kubectl -n zy-test get po -o wide 去掉污点 1kubectl taint no k8snode1 memory=low:NoExecute- 查看pod运行情况 亲和力Affinity亲和力 类似于nodeSelector，它使你可以根据节点上的标签来约束Pod可以调度到那些节点上: requireDuringSchedulingIgnoredDuringExecution: 调度器只有在规则被满足的时候才能执行调度。 preferredDuringSchedulingIgnoredDuringExecution: 调度器会尝试寻找满足对应规则的节点。如果找不到匹配的节点，调度器仍然会调度该pod. 匹配类型: In 满足一个就行 NotIn 一个都不能满足，反亲和性 Exists 只要存在，就满足 DoesNotExist 只有存在，才满足 Gt 必须要大于节点上 数值才满足 Lt 必须小于节点上的数值才满足 节点的亲和性 删除master的污点标签，让master也可以调度 Taints: node-role.kubernetes.io/master:NoSchedule 1kubectl taint no node node-role.kubernetes.io/master:NoSchedule- 给k8snode1节点打标签 1kubectl label no k8snode1 kubernetes.io/os=linux 12kubectl label no node label-1=key-1kubectl label no k8snode1 label-2=key-2 编辑deploy 1kubectl -n zy-test edit deploy nginx-deploy 123456789101112131415161718192021222324252627282930template: metadata: creationTimestamp: null labels: app: nginx-deploy spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/os operator: In values: - linux preferredDuringSchedulingIgnoredDuringExecution: - weight: 1 preference: matchExpressions: - key: label-1 operator: In values: - key-1 - weight: 50 # 权重越高，后续调到这个节点的可能更高 preference: matchExpressions: - key: label-2 operator: In values: - key-2 编辑完成后查看pod，此时pod都运行在了k8snode1节点上 1kubectl -n zy-test get po -o wide 修改标签的权重 1kubectl -n zy-test edit deploy nginx-deploy 1234567891011121314- weight: 50 preference: matchExpressions: - key: label-1 operator: In values: - key-1- weight: 1 # 权重越高，后续调到这个节点的可能更高 preference: matchExpressions: - key: label-2 operator: In values: - key-2 查看pod状态kubectl -n zy-test get po -o wide， 最终pod都运行到了master节点上 修改标签的亲和性 1kubectl -n zy-test edit deploy nginx-deploy 1234567891011121314- weight: 1 preference: matchExpressions: - key: label-1 operator: NotIn values: - key-1- weight: 50 # 权重越高，后续调到这个节点的可能更高 preference: matchExpressions: - key: label-2 operator: In values: - key-2 查看pod状态kubectl -n zy-test get po -o wide，最终节点又重新部署到了node节点上 Pod的亲和力 参数说明 12podAffinity: 将与指定pod亲和力相匹配的pod部署在同一个节点。podAntiAffinity: 根据策略尽量部署或不部署到一起。 给节点打标签 12kubectl label no k8snode1 topology.kubernetes.io/zone=Vkubectl label no node topology.kubernetes.io/zone=R 确认标签 1kubectl get no --show-labels 创建yml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869---apiVersion: v1kind: Podmetadata: name: with-pod-affinity-s1 namespace: &quot;zy-test&quot; labels: security: S1 topology.kubernetes.io/zone: Rspec: affinity: podAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: security operator: In values: - S1 topologyKey: topology.kubernetes.io/zone podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: security operator: In values: - S2 topologyKey: topology.kubernetes.io/zone containers: - name: with-pod-affinity image: registry.k8s.io/pause:2.0---apiVersion: v1kind: Podmetadata: name: with-pod-affinity-s2 namespace: &quot;zy-test&quot; labels: security: S2 topology.kubernetes.io/zone: Vspec: affinity: podAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: security operator: In values: - S1 topologyKey: topology.kubernetes.io/zone podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: security operator: In values: - S2 topologyKey: topology.kubernetes.io/zone containers: - name: with-pod-affinity image: registry.k8s.io/pause:2.0 查看pod状态，pod都被调度到了k8snode1节点上。 1kubectl -n zy-test get po -o wide 删除pod 1kubectl -n zy-test delete po with-pod-affinity-s1 with-pod-affinity-s2 删除k8snode1节点上的标签topology.kubernetes.io/zone 1kubectl label no k8snode1 topology.kubernetes.io/zone- 重新创建pod 1kubectl create -f affinity-pod.yml pod已经被调度到了node节点 1kubectl -n zy-test get po -o wide","categories":[{"name":"Kubernetes实战","slug":"Kubernetes实战","permalink":"http://jimoblivion.github.io/categories/Kubernetes%E5%AE%9E%E6%88%98/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://jimoblivion.github.io/tags/k8s/"}]},{"title":"Kubernetes实战笔记（十七）：initC初始化容器","slug":"Kubernetes实战笔记（十七）：initC初始化容器","date":"2023-07-21T03:18:43.000Z","updated":"2023-07-21T10:57:30.212Z","comments":true,"path":"2023/07/21/Kubernetes实战笔记（十七）：initC初始化容器/","link":"","permalink":"http://jimoblivion.github.io/2023/07/21/Kubernetes%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0%EF%BC%88%E5%8D%81%E4%B8%83%EF%BC%89%EF%BC%9AinitC%E5%88%9D%E5%A7%8B%E5%8C%96%E5%AE%B9%E5%99%A8/","excerpt":"","text":"initC 在真正的容器启动之前，先启动initContainer，再初始化容器中完成真实容器所需要的初始化操作，完成后再次启动真实的容器。 相对于postStart来说，首先initContainer能够保证一定在EntryPoint之前执行，而postStart不能，其次postStart更适合去执行一些命令操作，而initController实际就是一个容器，可以在其他基础容器环境下执行更复杂的初始化功能。 查看当前deploy 1kubectl -n zy-test get deploy 编辑当前deploy 1kubectl -n zy-test edit deploy nginx-deploy 1234567891011template: metadata: creationTimestamp: null labels: app: nginx-deploy spec: initContainers: - image: nginx:1.7.9 imagePullPolicy: IfNotPresent command: [&quot;sh&quot;, &quot;-c&quot;, &quot;sleep 10; echo &#x27;inited&#x27; &gt;&gt; /.init&quot;] name: init-test 测试 保存后查看我们Pod状态，发现新创建的Pod的STATUS时init:0&#x2F;1的状态 查看最新的容器 1kubectl -n zy-test get po 进入容器内部 1kubectl -n zy-test exec -it nginx-deploy-787859bfbd-j2v5p -- sh 并没有看到我们的init目录，原因是文件随着初始化容器一起被销毁了。","categories":[{"name":"Kubernetes实战","slug":"Kubernetes实战","permalink":"http://jimoblivion.github.io/categories/Kubernetes%E5%AE%9E%E6%88%98/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://jimoblivion.github.io/tags/k8s/"},{"name":"initC","slug":"initC","permalink":"http://jimoblivion.github.io/tags/initC/"}]},{"title":"Kubernetes实战笔记（十六）：CronJob定时任务","slug":"Kubernetes实战笔记（十六）：CronJob定时任务","date":"2023-07-21T03:12:42.000Z","updated":"2023-07-21T03:35:40.501Z","comments":true,"path":"2023/07/21/Kubernetes实战笔记（十六）：CronJob定时任务/","link":"","permalink":"http://jimoblivion.github.io/2023/07/21/Kubernetes%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0%EF%BC%88%E5%8D%81%E5%85%AD%EF%BC%89%EF%BC%9ACronJob%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1/","excerpt":"","text":"CronJob123在K8s中周期性运行计划任务，与linux中的crontab相同注意：CronJob执行的事件是controller-manager的时间，所以一定要确保controller-manager时间时准确的。 创建 创建yml 123456789101112131415161718192021222324apiVersion: batch/v1kind: CronJobmetadata: name: cron-job-test namespace: &quot;zy-test&quot;spec: concurrencyPolicy: Allow failedJobsHistoryLimit: 1 successfulJobsHistoryLimit: 3 suspend: false schedule: &quot;* * * * *&quot; jobTemplate: spec: template: spec: containers: - name: busybox image: busybox:1.28 imagePullPolicy: IfNotPresent command: - /bin/sh - -c - date; echo Hello from the Kubernetes cluster restartPolicy: OnFailure 1kubectl apply -f cron-job-po.yml 稍等片刻查看pod 1kubectl -n zy-test get po 查看pod日志 等两分钟后再次查看Pod,Pod会维持3个然后销毁最老的Pod并且启动新的Pod","categories":[{"name":"Kubernetes实战","slug":"Kubernetes实战","permalink":"http://jimoblivion.github.io/categories/Kubernetes%E5%AE%9E%E6%88%98/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://jimoblivion.github.io/tags/k8s/"},{"name":"CronJob","slug":"CronJob","permalink":"http://jimoblivion.github.io/tags/CronJob/"}]},{"title":"Kubernetes实战笔记（十五）：StorageClass存储类","slug":"Kubernetes实战笔记（十五）：StorageClass存储类","date":"2023-07-21T02:23:43.000Z","updated":"2023-07-21T03:12:59.558Z","comments":true,"path":"2023/07/21/Kubernetes实战笔记（十五）：StorageClass存储类/","link":"","permalink":"http://jimoblivion.github.io/2023/07/21/Kubernetes%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0%EF%BC%88%E5%8D%81%E4%BA%94%EF%BC%89%EF%BC%9AStorageClass%E5%AD%98%E5%82%A8%E7%B1%BB/","excerpt":"","text":"说明随着我们线上环境应用越来越多，pv申领越来越麻烦，所以使用StorageClass来动态制备PV。 Provisioner：制备器。每个StorageClass都有一个制备器，用来约定使用哪个卷插件制备PV。 动态创建NFS-PV首先创建rbac权限 新建yaml 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061apiVersion: v1kind: ServiceAccountmetadata: name: nfs-client-provisioner namespace: kube-system---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata: name: nfs-client-provisioner-runner namespace: kube-systemrules: - apiGroups: [&quot;&quot;] resources: [&quot;persistentvolume&quot;] verbs: [&quot;get&quot;,&quot;list&quot;,&quot;watch&quot;,&quot;create&quot;,&quot;delete&quot;] - apiGroups: [&quot;&quot;] resources: [&quot;persistentvolumeclaims&quot;] verbs: [&quot;get&quot;,&quot;list&quot;,&quot;watch&quot;,&quot;update&quot;] - apiGroups: [&quot;storage.k8s.ioi&quot;] # ！！拼写错误，之后会进行问题排查 resources: [&quot;storageclasses&quot;] verbs: [&quot;get&quot;,&quot;list&quot;,&quot;watch&quot;] - apiGroups: [&quot;&quot;] resources: [&quot;events&quot;] verbs: [&quot;create&quot;,&quot;update&quot;,&quot;patch&quot;]---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: name: run-nfs-client-provisioner namespace: kube-systemsubjects: - kind: ServiceAccount name: nfs-client-provisioner namespace: &quot;zy-test&quot;roleRef: kind: ClusterRole name: nfs-client-provisioner-runner apiGroup: rbac.authorization.k8s.io---apiVersion: rbac.authorization.k8s.io/v1kind: Rolemetadata: name: leader-locking-nfs-client-provisioner namespace: kube-systemrules: - apiGroups: [&quot;&quot;] resources: [&quot;endpoints&quot;] verbs: [&quot;get&quot;,&quot;list&quot;,&quot;watch&quot;,&quot;create&quot;,&quot;update&quot;,&quot;patch&quot;]---apiVersion: rbac.authorization.k8s.io/v1kind: RoleBindingmetadata: name: leader-locking-nfs-client-provisioner namespace: kube-systemsubjects: - kind: ServiceAccount name: nfs-client-provisionerroleRef: kind: Role name: leader-locking-nfs-client-provisioner apiGroup: rbac.authorization.k8s.io 创建权限 1kubectl apply -f nfs-provisioner-rbac.yml 创建deployment 1234567891011121314151617181920212223242526272829303132333435363738apiVersion: apps/v1kind: Deploymentmetadata: name: nfs-client-provisioner namespace: kube-system labels: app: nfs-client-provisionerspec: replicas: 1 strategy: type: Recreate # 停止旧版本，启动新版本 selector: matchLabels: app: nfs-client-provisioner template: metadata: labels: app: nfs-client-provisioner spec: serviceAccountName: nfs-client-provisioner containers: - name: nfs-client-provisioner image: quay.io/external_storage/nfs-client-provisioner:latest volumeMounts: - name: nfs-client-root mountPath: /persistentvolumes env: - name: PROVISIONER_NEW # 制备器的名字 value: fuseim.pri/ifs - name: NFS_SERVER value: 192.168.1.21 - name: NFS_PATH value: /data/nfs/rw volumes: - name: nfs-client-root nfs: server: 192.168.1.21 path: /data/nfs/rw 1kubectl apply -f nfs-provisioner-deployment.yml 查看Pod发现出现错误 CrashLoopBackOff 对Pod进行描述，看不出具体原因 查看deployment 对deployment进行描述 1kubectl -n kube-system describe nfs-client-provisioner MinimumReplicasUnavailable最小副本不可用，也就是项目的配额超出。 MinimumReplicasUnavailable是指在Kubernetes中，当一个Deployment的ReplicaSet的可用副本数小于期望的副本数时，就会出现这个问题。解决方法是检查Pod的状态，以及检查Pod是否有足够的资源。如果Pod没有足够的资源，可以通过增加Pod的资源限制来解决这个问题。 查看rs，发现副本的ready数量为0 1kubectl -n kube-system get rs | grep nfs 解决办法: 启动时执行命令tail -f /dev/null，防止容器崩溃 12345678910template: metadata: labels: app: nfs-client-provisioner spec: serviceAccountName: nfs-client-provisioner containers: - name: nfs-client-provisioner image: quay.io/external_storage/nfs-client-provisioner:latest command: [ &quot;/bin/sh&quot;, &quot;-ce&quot;, &quot;tail -f /dev/null&quot; ] 创建statefulSet 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950---apiVersion: v1kind: Servicemetadata: name: nginx-sc namespace: &quot;zy-test&quot; labels: app: nginx-scspec: type: NodePort ports: - name: web port: 80 protocol: TCP selector: app: nginx-sc---apiVersion: apps/v1kind: StatefulSetmetadata: name: nginx-sc namespace: &quot;zy-test&quot;spec: replicas: 1 serviceName: &quot;nginx-sc&quot; selector: matchLabels: app: nginx-sc template: metadata: labels: app: nginx-sc spec: containers: - image: nginx:1.7.9 name: nginx-sc imagePullPolicy: IfNotPresent volumeMounts: - mountPath: /usr/share/nginx/html name: nginx-sc-test-pvc volumeClaimTemplates: # 直接配置PVC和PV - metadata: name: nginx-sc-test-pvc spec: storageClassName: managed-nfs-storage accessModes: - ReadWriteMany # 多个节点同时读写 resources: requests: storage: 2Gi 1kubectl apply -f nfs-sc-demo-statefulset.yml 问题排查问题1 查看pod的状态 kubectl -n zy-test get po，发现pod的状态一直处于pending 对pod进行描述 1kubectl -n zy-test describe po nginx-sc-0 1kubectl -n zy-test get pvc 此时PVC是Pending，PV没有创建 分析 12345678910111213141516由于k8s 1.20版本以后出于对性能等原因，k8s把SeflLink这个功能禁用了。两种解决方法： 1. 修改apiserver配置文件（不推荐） 需要在/etc/kubernetes/manifests/kube-apiserver.yaml 添加参数增加 --feature-gates=RemoveSelfLink=false 2. 使用不需要SelfLink的Provisioner 需要将镜像地址改为阿里云的地址；另外将制备器的名字PROVISIONER_NEW改为ROVISIONER_NAME containers: - name: nfs-client-provisioner #image: gcr.io/k8s-staging-sig-storage/nfs-subdir-external-provisioner:v4.0.0 image: registry.cn-beijing.aliyuncs.com/xngczl/nfs-subdir-external-provisione:v4.0.0 volumeMounts: - name: nfs-client-root mountPath: /persistentvolumes env: - name: PROVISIONER_NAME ## Provisioner的名称,以后设置的storageclass要和这个保持一致 value: fuseim.pri/ifs 我们使用方法2。如果镜像还是无法拉取，解决办法是将镜像拉取到docker本地仓库，并且打标签让k8s可以识别到，最后要设置镜像拉取方式从本地获取。 检查Pod的状态已经可以正常运行 创建StorageClass 123456789apiVersion: storage.k8s.io/v1kind: StorageClassmetadata: name: managed-nfs-storageprovisioner: fuseim.pri/ifsparameters: archiveOnDelete: &quot;false&quot;reclaimPolicy: RetainvolumeBindingMode: Immediate 1kubectl apply -f nfs-sc.yml 查看创建的StorageClass 1kubectl -n zy-test get sc 创建StatefulSet 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748---apiVersion: v1kind: Servicemetadata: name: nginx-sc labels: app: nginx-scspec: type: NodePort ports: - name: web port: 80 protocol: TCP selector: app: nginx-sc---apiVersion: apps/v1kind: StatefulSetmetadata: name: nginx-scspec: replicas: 1 serviceName: &quot;nginx-sc&quot; selector: matchLabels: app: nginx-sc template: metadata: labels: app: nginx-sc spec: containers: - image: nginx:1.7.9 name: nginx-sc imagePullPolicy: IfNotPresent volumeMounts: - mountPath: /usr/share/nginx/html name: nginx-sc-test-pvc volumeClaimTemplates: # 直接配置PVC和PV - metadata: name: nginx-sc-test-pvc spec: storageClassName: managed-nfs-storage accessModes: - ReadWriteMany # 多个节点同时读写 resources: requests: storage: 1Gi 1kubectl apply -f nfs-sc-demo-statefulset.yml 问题2 创建成功后查看PVC状态，发现PVC处于pending状态。 1kubectl -n default get pvc 1kubectl -n default describe pvc nginx-sc-test-pvc-nginx-sc-0 Waiting for a volume to be created, either by external provisioner “fuseim.pri&#x2F;ifs” or manually created by system administrator 解决方案，修改rabc权限配置 12- apiGroups: [&quot;storage.k8s.io&quot;] # 修改 resources: [&quot;storageclasses&quot;] 或者使用以下权限进行替换 12345678910111213141516171819202122232425262728apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRole # 创建集群角色metadata: name: nfs-client-provisioner-runner# 角色权限rules: - apiGroups: [&quot;&quot;] resources: [&quot;persistentvolumes&quot;] verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;delete&quot;] - apiGroups: [&quot;&quot;] resources: [&quot;persistentvolumeclaims&quot;] verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;update&quot;] - apiGroups: [&quot;storage.k8s.io&quot;] resources: [&quot;storageclasses&quot;] verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;] - apiGroups: [&quot;&quot;] resources: [&quot;events&quot;] verbs: [&quot;watch&quot;, &quot;create&quot;, &quot;update&quot;, &quot;patch&quot;] - apiGroups: [&quot;&quot;] resources: [&quot;services&quot;] verbs: [&quot;get&quot;] - apiGroups: [&quot;extensions&quot;] resources: [&quot;podsecuritypolicies&quot;] resourceNames: [&quot;nfs-provisioner&quot;] verbs: [&quot;use&quot;] - apiGroups: [&quot;&quot;] resources: [&quot;endpoints&quot;] verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;update&quot;, &quot;patch&quot;] 更新权限 1kubectl apply -f nfs-provisioner-rbac.yml 重启服务即可 删除多余PV 发现多余的PV 使用命令进行删除 发现进程卡死，无法删除 解决办法，使用如下命令强制删除PV 1kubectl patch pv pvc-c9ddb982-45f7-4ffa-94bd-33da4749e468 -p &#x27;&#123;&quot;metadata&quot;:&#123;&quot;finalizers&quot;:null&#125;&#125;&#x27; 查看PV已经被成功删除","categories":[{"name":"Kubernetes实战","slug":"Kubernetes实战","permalink":"http://jimoblivion.github.io/categories/Kubernetes%E5%AE%9E%E6%88%98/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://jimoblivion.github.io/tags/k8s/"},{"name":"StorageClass","slug":"StorageClass","permalink":"http://jimoblivion.github.io/tags/StorageClass/"}]},{"title":"Kubernetes实战笔记（十四）：PV和PVC","slug":"Kubernetes实战笔记（十四）：PV和PVC","date":"2023-07-20T12:28:39.000Z","updated":"2023-07-21T03:11:21.133Z","comments":true,"path":"2023/07/20/Kubernetes实战笔记（十四）：PV和PVC/","link":"","permalink":"http://jimoblivion.github.io/2023/07/20/Kubernetes%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0%EF%BC%88%E5%8D%81%E5%9B%9B%EF%BC%89%EF%BC%9APV%E5%92%8CPVC/","excerpt":"","text":"原因 由于不同服务可能使用不同的持久化实现方案，会导致混乱，没有统一的标准。 持久卷（Persistent Volume）是集群中的一块存储，可以由管理员提前制备，或者使用Storage Class来动态制备。PV持久卷和普通的Volume一样，也是使用卷插件来实现的，只是他们拥有独立于任何使用PV的Pod的生命周期。 持久卷申领（Persistent Volume Claim）表达用户对存储的请求。与Pod类似，Pod会消耗节点资源，但是PVC申领会消耗PV资源。 PV卷能够以ReadWriteOnce、ReadOnlyMany或ReadWriteMang模式之一来挂载。 构建静态构建集群管理员创建若干PV卷，这些卷对象带有真实存储的细节信息，并且对集群用户可见。PV卷对象存在于Kubernetes API中，可供消费者使用 动态构建如果进群中已经有的PV无法满足PVC的需求，那么集群会根据PVC自动构建一个PV，该操作是通过StroageClass实现的。想要实现这个操作，前提是PVC必须设置StorageClass，否者会无法动态构建该PV，可以通过DefaultStorageClass来实现PV的构建。 绑定当用户创建了一个PVC对象后，主节点会监测新的PVC对象，并且寻找与之匹配的PV卷，找到PV卷后将二者绑定在一起。如果找不到对应的PV，则需要看PVC是否设置StorageClass来决定是否动态创建PV，若没有配置，PVC就会一直处于未绑定的状态，直到有与之匹配的PV后才会申领绑定关系。 使用Pod将PVC当作存储卷来使用，集群会通过PVC找到绑定的PV， 并为Pod挂载该卷。Pod一旦使用PVC绑定PV后，为了保护数据，避免数丢失问题，PV对象会收到保护，在系统中无法被删除。 回收策略当用户不再使用其存储卷时，他们可以从API中将PVC对象删除，从而允许该资源被回收再利用。PV对象的回收策略告诉集群，当其被从申领中释放时如何处理该数据卷。目前，数据卷可以被**Retained(保留)、Recycled(回收)或Deleted(删除)**。 保留12345回收策略Retain使得用户可以手动回收资源。当PVC对象别删除时，PV卷仍然存在，对应的数据卷被视为“已释放”。 由于卷上仍然存着前一申领人的数据，该卷还不能用于其他申领。管理员可以通过下面步骤来手动回收该卷。a. 删除PV对象。与之相关的、位于外部基础设施的存储资产在PV删除之后仍然存在。b. 根据情况，手动清除所有关联的存储资产上的数据c. 手动删除所关联的存储资产 删除123对于支持Delete回收策略的卷插件，删除动作会将PV对象从Kubernetes中移除，同时也会从外部基础设施中移除所更换连的存储资产。动态制备的卷会继承其StorageClass中设置的回收策略，该策略默认为Delete。管理员需要根据用户的期望来配置StorageClass： 否则PV卷被创建之后必须要被编辑或者修补。 回收12回收策略Recycle已经被废弃。取而代之的建议方案时使用动态制备。如果下层的卷插件支持，回收策略Recycle会在卷上执行一些基本的擦除（rm -rf /thevolume/*）操作，之后允许该卷用于新的PVC申领。 PV的状态Available: 空闲，未被绑定Bound: 已经被PVC绑定Released: PVC被删除，资源已回收，但是PV未被重新使用Failed: 自动回收失败 创建pv 12345678910111213141516171819apiVersion: v1kind: PersistentVolumemetadata:name: pv-001namespace: &quot;zy-test&quot;spec:capacity: storage: 2GivolumeMode: FilesystemaccessModes: - ReadWriteManypersistentVolumeReclaimPolicy: RetainstorageClassName: slowmountOptions: - hard - nfsvers=4.2nfs: path: /data/nfs/rw/test-pv server: 192.168.1.21 创建共享目录 1mkdir -p /data/nfs/rw/test-pv 1kubectl create -f nfs-pv.yml 查看pv已经被构建成功 1kubectl -n zy-test get pv 构建PVC 12345678910111213apiVersion: v1kind: PersistentVolumeClaimmetadata:name: nfs-pvcnamespace: &quot;zy-test&quot;spec:accessModes: - ReadWriteMany # 权限需要和对应的pv相同volumeMode: Filesystemresources: requests: storage: 2Gi # 资源要小于pv的storageClassName: slow # 名字要和对应的pv相同 1kubectl create -f nfs-pvc.yml 查看pvc已经被构建成功，并且成功绑定到pv-001 1kubectl -n zy-test get pvc Pod绑定PVC，在Pod的挂载容器配置中，增加PVC挂载 1234567891011121314151617apiVersion: v1kind: Podmetadata:name: test-pvc-pdnamespace: &quot;zy-test&quot;spec:containers:- image: nginx:1.7.9 imagePullPolicy: IfNotPresent name: nginx-volume volumeMounts: - mountPath: /test-pd # 挂载到容器的哪个目录 name: test-volume # 挂载到哪个volumevolumes:- name: test-volume persistentVolumeClaim: claimName: nfs-pvc # 通过name绑定到指定的PVC 1kubectl create -f nginx-pvc-pd.yml 查看pod信息 1kubectl -n zy-test get po -o wide 测试 测试文件同步 123cd /data/nfs/rw/test-pv/echo &quot;success...^_^&quot; &gt; index.htmlcurl 10.244.249.34 文件同步成功","categories":[{"name":"Kubernetes实战","slug":"Kubernetes实战","permalink":"http://jimoblivion.github.io/categories/Kubernetes%E5%AE%9E%E6%88%98/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://jimoblivion.github.io/tags/k8s/"},{"name":"pv","slug":"pv","permalink":"http://jimoblivion.github.io/tags/pv/"},{"name":"pvc","slug":"pvc","permalink":"http://jimoblivion.github.io/tags/pvc/"}]},{"title":"Kubernetes实战笔记（十三）：NFS挂载","slug":"Kubernetes实战笔记（十三）：NFS挂载","date":"2023-07-20T12:00:25.000Z","updated":"2023-07-20T12:54:10.243Z","comments":true,"path":"2023/07/20/Kubernetes实战笔记（十三）：NFS挂载/","link":"","permalink":"http://jimoblivion.github.io/2023/07/20/Kubernetes%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0%EF%BC%88%E5%8D%81%E4%B8%89%EF%BC%89%EF%BC%9ANFS%E6%8C%82%E8%BD%BD/","excerpt":"","text":"NFS说明nfs卷能将NFS(网络文件系统)挂载到你的Pod中。不像emptyDir那样会删除Pod的同时也会被删除。nfs卷的内容在删除Pod时会被保存，卷只是被卸载。 这意味着nfs卷可以被预先填充数据，并且这些数据可以在pod之间共享。 总结：NFS实现了容器之间的数据共享，并且实现了持久化，但是因为网络IO和磁盘IO等会导致效率比较低。 安装在使用nfs的服务器上分别安装• 安装nfs yum install nfs-utils -y• 启动nfs systemctl start nfs-server• 查看nfs的版本 cat /proc/fs/nfsd/versions• 在k8snode1节点上创建共享目录 mkdir -p /data/nfs cd /data/nfs/ mkdir rw mkdir ro• 设置共享目录 export vim /etc/exports• 配置共享目录和网段 /data/nfs/rw 192.168.1.0/24(rw,sync,no_subtree_check,no_root_squash) /data/nfs/ro 192.168.1.0/24(ro,sync,no_subtree_check,no_root_squash)• 重新加载nfs exportfs -f systemctl reload nfs-server• 到node节点安装nfs-utils并进行测试 mkdir -p /mnt/nfs/rw mkdir -p /mnt/nfs/ro mount -t nfs 192.168.1.21:/data/nfs/rw /mnt/nfs/rw mount -t nfs 192.168.1.21:/data/nfs/ro /mnt/nfs/ro 测试• 在node节点共享读写&#x2F;rw目录中创建文件 cd /mnt/nfs/rw/ touch README.md• 查看k8snode1节点中共享目录文件，发现README.md文件已经同步成功 cd /data/nfs/rw/• 在node节点只读&#x2F;ro目录中创建文件 cd /mnt/nfs/ro/• 文件创建失败，提示： touch: cannot touch ‘test’: Read-only file system 实现文件系统挂载 创建yml 123456789101112131415161718apiVersion: v1kind: Podmetadata: name: nfs-test-pd1 namespace: &quot;zy-test&quot;spec: containers: - image: nginx:1.7.9 name: test-container volumeMounts: - mountPath: /usr/share/nginx/html name: test-volume volumes: - name: test-volume nfs: server: 192.168.1.21 path: /data/nfs/rw/www/share readOnly: false 创建共享目录和文件 12mkdir -p /data/nfs/rw/www/shareecho &#x27;&lt;h1&gt;hello&lt;/h1&gt;&#x27; &gt; /data/nfs/rw/www/share/index.html 创建pod 1kubectl create -f nginx-nfs-po.yml 查看pod的ip，并且访问 修改共享目录中文件内容 1echo &#x27;&lt;h1&gt;success&lt;/h1&gt;&#x27; &gt; /data/nfs/rw/www/share/index.html","categories":[{"name":"Kubernetes实战","slug":"Kubernetes实战","permalink":"http://jimoblivion.github.io/categories/Kubernetes%E5%AE%9E%E6%88%98/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://jimoblivion.github.io/tags/k8s/"},{"name":"nfs","slug":"nfs","permalink":"http://jimoblivion.github.io/tags/nfs/"}]},{"title":"Kubernetes实战笔记（十二）：持久化存储","slug":"Kubernetes实战笔记（十二）：持久化存储","date":"2023-07-20T10:35:19.000Z","updated":"2023-07-20T12:19:56.060Z","comments":true,"path":"2023/07/20/Kubernetes实战笔记（十二）：持久化存储/","link":"","permalink":"http://jimoblivion.github.io/2023/07/20/Kubernetes%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0%EF%BC%88%E5%8D%81%E4%BA%8C%EF%BC%89%EF%BC%9A%E6%8C%81%E4%B9%85%E5%8C%96%E5%AD%98%E5%82%A8/","excerpt":"","text":"创建 创建yml 1234567891011121314151617apiVersion: v1kind: Podmetadata: name: test-volume-pd namespace: &quot;zy-test&quot;spec: containers: - image: nginx:1.7.9 name: nginx-volume volumeMounts: - mountPath: /test-pd name: test-volume volumes: - name: test-volume hostPath: path: /usr/local/docker/data/ type: DirectoryOrCreate HostPath说明将节点上的文件或目录挂载到Pod上，此时该目录会变成持久化存储目录，即使Pod被删除后重启，也可以重新加载到该目录，该目录下的文件不会丢失。 Type: 类型 $空字符串$： 默认类型，不做任何检查$DirectoryOrCreate$: 如果给定的path不存在，就创建一个755的空目录$Directory$: 这个目录必须存在$FileOrCreate$: 如果给定的文件不存在，则创建一个空文件，权限为644$File$: 这个文件必须存在$Socket$: UNIX 套接字，必须存在$CharDevice$: 字符设备，必须存在$BlockDevice$: 块设备，比哦徐存在 测试 进入容器内部目录 ，kubectl -n zy-test exec -it test-volume-pd -- sh 目录挂载成功，我们从外部添加文件 touch new.txt 再容器内部可以查看到新增的文件 同样，在容器内部我们新增文件 节点上文件也被同步，这样我们就实现了文件的共享 EmptyDir说明主要用于一个Pod中不同的Container共享数据使用的，由于只是在Pod内部使用，因此与其他volume比较大的区别是，当Pod如果被删除了，那么emptyDir也会被删除。 存储介质可以是任意类型，可以将emptyDir.medium设置为Memory让k8s使用tmpfs(内存支持文件系统)，速度比较块，但是重启tmpfs节点时，数据会被清理，且设置的大小会计入到Container的内存限制中。 测试 创建yml 12345678910111213141516171819202122apiVersion: v1kind: Podmetadata: name: empty-dir-pd namespace: &quot;zy-test&quot;spec: containers: - image: alpine name: nginx-emptydir1 command: [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;sleep 3600;&quot;] volumeMounts: - mountPath: /cache name: cache-volume - image: alpine name: nginx-emptydir2 command: [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;sleep 3600;&quot;] volumeMounts: - mountPath: /opt name: cache-volume volumes: - name: cache-volume emptyDir: &#123;&#125; 创建成功后进入容器nginx-emptydir1内部 1kubectl -n zy-test exec -it empty-dir-pd -c nginx-emptydir1 -- sh 可以看到我们挂载的目录&#x2F;cache 测试在共享目录创建文件 touch test.txt 进入容器nginx-emptydir2内部 1kubectl -n zy-test exec -it empty-dir-pd -c nginx-emptydir2 -- sh 可以看到我们挂载的目录&#x2F;opt，并且文件实现了同步","categories":[{"name":"Kubernetes实战","slug":"Kubernetes实战","permalink":"http://jimoblivion.github.io/categories/Kubernetes%E5%AE%9E%E6%88%98/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://jimoblivion.github.io/tags/k8s/"},{"name":"HostPath","slug":"HostPath","permalink":"http://jimoblivion.github.io/tags/HostPath/"},{"name":"EmptyDir","slug":"EmptyDir","permalink":"http://jimoblivion.github.io/tags/EmptyDir/"}]},{"title":"Kubernetes实战笔记（十一）：配置与存储","slug":"Kubernetes实战笔记（十一）：配置与存储","date":"2023-07-20T08:48:34.000Z","updated":"2023-07-20T12:05:46.521Z","comments":true,"path":"2023/07/20/Kubernetes实战笔记（十一）：配置与存储/","link":"","permalink":"http://jimoblivion.github.io/2023/07/20/Kubernetes%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0%EF%BC%88%E5%8D%81%E4%B8%80%EF%BC%89%EF%BC%9A%E9%85%8D%E7%BD%AE%E4%B8%8E%E5%AD%98%E5%82%A8/","excerpt":"","text":"ConfigMap创建ConfigMap 创建文件 1234mkdir test# 添加自定义属性配置：vim ./test/db.propertiesvim ./test/redis.properties 创建cm 1kubectl -n zy-test create configmap test-dir-config --from-file=./test/ 查看配置信息 1kubectl -n zy-test describe cm test-dir-config ConfigMap的创建方式 指定文件，并且自定义文件名来创建configMap 1kubectl -n zy-test create cm test-file-redis-config --from-file=redis.conf=./test/redis.properties 没有文件名的方式创建configMap 1kubectl -n zy-test create configmap test-key-value-config --from-literal=username=root --from-literal=passwd=123 测试ConfigMap使用 创建cm 1kubectl -n zy-test create configmap test-env-config --from-literal=JAVA_OPTS_TEST=&#x27;-Xms512m -Xmx512m&#x27; --from-literal=APP_NAME=spring 新建yml，并且引用configMap中的项 12345678910111213141516171819202122232425262728293031323334apiVersion: v1kind: Podmetadata: name: test-env-cm namespace: zy-testspec: containers: - name: env-test image: alpine command: [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;env;sleep 3600&quot;] imagePullPolicy: IfNotPresent env: - name: JAVA_VM_OPTS valueFrom: configMapKeyRef: name: test-env-config key: JAVA_OPTS_TEST - name: APP valueFrom: configMapKeyRef: name: test-env-config key: APP_NAME volumeMounts: - name: db-config mountPath: &quot;/usr/local/mysql/conf&quot; readOnly: true volumes: - name: db-config configMap: name: test-dir-config items: - key: &quot;db.properties&quot; path: &quot;db.properties&quot; restartPolicy: Never 查看日志 1kubectl -n zy-test logs -f test-env-cm 1kubectl create -f env-test-pod.yml 进入容器，查看配置挂载的目录的配置文件 1kubectl -n zy-test exec -it test-env-cm -- sh 加密数据配置 Secret注意：如果要加密的字符中包含了特殊字符，需要使用转义符转义，例如$转义后为\\\\$ 也可以对特殊字符使用单引号描述，这样就不需要转义例如1$289*!转换为&#39;1$289*!&#39; 数据加密 1kubectl -n zy-test create secret docker-registry harbor-secret --docker-username=admin --docker-password=admin --docker-email=sam@qq.com 查看密钥 1kubectl -n zy-test edit secret harbor-secret 使用base64进行解密，可以看到初始值 1echo &#x27;eyJhdXRocyI6eyJodHRwczovL2luZGV4LmRvY2tlci5pby92MS8iOnsidXNlcm5hbWUiOiJhZG1pbiIsInBhc3N3b3JkIjoiYWRtaW4iLCJlbWFpbCI6InNhbUBxcS5jb20iLCJhdXRoIjoiWVdSdGFXNDZZV1J0YVc0PSJ9fX0=&#x27; | base64 --decode SubPath的使用使用SubPath的原因 查看容器 kubectl -n zy-test get po 进入容器 kubectl -n zy-test exec -it nginx-deploy-7b49587448-vqb7l -- sh 找到容器中文件/etc/nginx/conf.d/nginx.conf 拷贝内容，新建文件到我们的目录中nginx.conf文件中 创建configMap kubectl -n zy-test create configmap nginx-conf-cm --from-file=./nginx.conf 编辑deploy，新增数据卷 kubectl -n zy-test edit deploy nginx-deploy 123456789101112131415161718192021spec: template: spec: containers: - image: nginx:1.7.9 imagePullPolicy: IfNotPresent name: nginx -- terminationMessagePath: /dev/termination-log terminationMessagePolicy: File volumeMounts: - name: nginx-conf # 数据卷的名称 mountPath: &#x27;/etc/nginx&#x27; # 希望把文件加载到容器中的目录 volumes: - name: nginx-conf configMap: name: nginx-conf-cm # configmap的name items: - key: nginx.conf path: nginx.conf dnsPolicy: ClusterFirst 查看容器状态，发现报错 kubectl -n zy-test get po 描述容器中的错误内容 kubectl -n zy-test describe po nginx-deploy-76459d6d66-fxmsg 进入容器查看文件是否加载，发现报错，原因是我们容器未启动成功。解决办法: 给容器加一个睡眠时间，让他不要挂掉。 编辑deployment kubectl -n zy-test edit deploy nginx-deploy 123456spec: containers: - image: nginx:1.7.9 imagePullPolicy: IfNotPresent name: nginx command: [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;nginx daemon off;sleep 3600&quot;] 查看当前pod 进入容器 kubectl -n zy-test exec -it nginx-deploy-5f986958cc-fh8wf -- bash 发现容器目录中其他文件都没有了，只剩下nginx.conf。原因是使用ConfigMap或Secret挂载到目录的时候，会将容器中源目录覆盖掉，此时我们可能只想覆盖目录中某一个文件， 所以要使用subpath。 使用SubPath 编辑deploy, &#x3D;&#x3D;#1&#x3D;&#x3D; 和 &#x3D;&#x3D;#2&#x3D;&#x3D;的路径要保持一致 1kubectl -n zy-test edit deploy nginx-deploy 1234567891011121314---- volumes: - configMap: defaultMode: 420 items: - key: nginx.conf path: etc/nginx/nginx.conf # 1 name: nginx-conf-cm name: nginx-conf--- volumeMounts: - mountPath: /etc/nginx/nginx.conf name: nginx-conf subPath: etc/nginx/nginx.conf #2 查看pod的运行状态 1kubectl -n zy-test get po 进入容器内部，文件正常 1kubectl -n zy-test edit deploy nginx-deploy ConfigMap热更新 如果更新configmap中的配置，会不会更新到pod中呢？ 1234567默认方式：会更新，更新周期是更新时间 + 缓存时间subPath: 不会更新变量形式：如果pod中一个变量是从configmap或secret中得到的，同样不会更新对于subPath的方式，我们可以取消subPath的使用，将配置文件挂载到一个不存在的目录，避免目录的覆盖，然后再利用软连接的形式，将该文件链接到目标位置。但是如果目标位置原本就有文件，可能无法创建软连接，此时可以基于postStart操作执行删除命令，将默认的文件删除即可。 修改cm 1kubectl -n zy-test edit cm test-dir-config 进入容器内部查看配置已经变更 1kubectl -n zy-test exec -it test-env-cm -- sh 使用replace替换，结合--dry-run参数。该参数的意思是打印yaml文件，但是不会将文件发送给apiServer，结合-oyaml输出yaml文件就可以得到一个配置好但是没有发给apiserver的文件，然后再结合replace监听控制台输出得到yaml数据即可实现替换。 查看确认当前配置 1kubectl -n zy-test create cm test-dir-config --from-file=./test/ --dry-run=client -o yaml 把前置作为输入 1kubectl -n zy-test create cm test-dir-config --from-file=./test/ --dry-run=client -o yaml | kubectl replace -f- 执行完成之后发现配置已经被替换为新的 不可变配置对于一些敏感服务的配置文件，在线上有时是不允许修改的，此时只需要再配置configMap的时候设置immutable:true即可 编辑configMap， 打开配置项 1kubectl -n zy-test edit cm test-dir-config 123456789apiVersion: v1data: db.properties: | username=root passwd=123 redis.properties: | ip=127.0.0.1 port=6379immutable: true 再次编辑配置项，修改属性值，此时已经不允许修改 1kubectl -n zy-test edit cm test-dir-config","categories":[{"name":"Kubernetes实战","slug":"Kubernetes实战","permalink":"http://jimoblivion.github.io/categories/Kubernetes%E5%AE%9E%E6%88%98/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://jimoblivion.github.io/tags/k8s/"},{"name":"ConfigMap","slug":"ConfigMap","permalink":"http://jimoblivion.github.io/tags/ConfigMap/"}]},{"title":"Kubernetes实战笔记（十）：Ingress","slug":"Kubernetes实战笔记（十）：Ingress","date":"2023-07-20T03:33:26.000Z","updated":"2023-07-20T11:03:07.423Z","comments":true,"path":"2023/07/20/Kubernetes实战笔记（十）：Ingress/","link":"","permalink":"http://jimoblivion.github.io/2023/07/20/Kubernetes%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0%EF%BC%88%E5%8D%81%EF%BC%89%EF%BC%9AIngress/","excerpt":"","text":"安装首先安装helm○下载二进制文件 https://get.helm.sh/helm-v3.12.0-linux-amd64.tar.gz○ 解压tar -zxvf helm-v3.12.0-linux-amd64.tar.gz○ 移动到目录 mv ./linux-amd64/helm /usr/local/bin/helm○ 测试helm version ○ 添加仓库 helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx○ 查看仓库列表 helm repo list 配置ingress○ 搜索ingress-nginx: helm search repo ingress-nginx ○ 下载安装包 helm pull ingress-nginx/ingress-nginx○ 将下载好的安装包解压 tar xf ingress-nginx-4.7.1.tgz○ 修改配置 vim ./ingress-nginx/values.yaml 12345678910111213controller: name: controller image: ## Keep false as default for now! chroot: false registry: registry.cn-hangzhou.aliyuncs.com # 修改 image: google_containers/nginx-ingress-controller # 修改 ## for backwards compatibility consider setting the full image url via the repository value below ## use *either* current default registry/image or repository format or installing chart by providing the values.yaml will fail ## repository: tag: &quot;v1.8.1&quot;# digest: sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd # 修改# digestChroot: sha256:e0d4121e3c5e39de9122e55e331a32d5ebf8d4d257227cb93ab54a1b912a7627 # 修改 1234567891011 patch: enabled: true image: registry: registry.cn-hangzhou.aliyuncs.com # 修改 image: google_containers/kube-webhook-controller # 修改 ## for backwards compatibility consider setting the full image url via the repository value below ## use *either* current default registry/image or repository format or installing chart by providing the values.yaml will fail ## repository: tag: v1.8.1# digest: sha256:543c40fd093964bc9ab509d3e791f9989963021f1e9e4c9c7b6700b02bfb227b # 修改 pullPolicy: IfNotPresent 1234567## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/##nodeSelector: kubernetes.io/os: linux ingress: &quot;true&quot; # 修改 增加选择器，如果node上有ingress=true就部署 ## Liveness and readiness probe values## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#container-probes 123# is mergedhostNetwork: true # 修改## Use host ports 80 and 443 123 # to keep resolving names inside the k8s network, use ClusterFirstWithHostNet.# dnsPolicy: ClusterFirst # 修改 dnsPolicy: ClusterFirstWithHostNet # 修改 12345678910ipFamilies: - IPv4ports: http: 80 https: 443targetPorts: http: http https: https#type: LoadBalancer # 修改 如果服务是云平台才用LoadBalancer type: ClusterIP # 修改 123456789admissionWebhooks: annotations: &#123;&#125; # ignore-check.kube-linter.io/no-read-only-rootfs: &quot;This deployment needs write access to root filesystem&quot;. ## Additional annotations to the admission webhooks. ## These annotations will be added to the ValidatingWebhookConfiguration and ## the Jobs Spec of the admission webhooks. enabled: false # 修改 # -- Additional environment variables to set 12# -- Use a `DaemonSet` or `Deployment` kind: DaemonSet # 修改部署配置 ○ 为ingress专门创建命名空间 kubectl create ns ingress-nginx 安装ingress○ 安装 helm install ingress-nginx -n ingress-nginx ./ingress-nginx ○ 为需要部署ingress的节点加上标签（让 Pod 调度到指定的节点）kubectl label node k8snode1 ingress=true ○ 查看节点 kubectl get node --show-labels ○ 可以看到ingress-nginx 部署到了node节点了 kubectl get all -n ingress-nginx 测试 创建yml 123456789101112131415161718apiVersion: networking.k8s.io/v1kind: Ingressmetadata: name: zy-nginx-ingress annotations: kubernetes.io/ingress.class: &quot;nginx&quot;spec: rules: - host: k8s.zy.cn http: paths: - pathType: Prefix backend: service: name: nginx-svc port: number: 80 path: /api 查看创建的ingress 1kubectl -o wide get ingress 给主机配置域名 1echo &quot;192.168.1.21 k8s.zy.cn&quot; &gt;&gt; /etc/hosts 描述ingress 1kubectl describe ingress zy-nginx-ingress 推测原因是不同namespace导致 删除ingress 1kubectl delete -f nginx-ig.yml 修改命名空间： ○ 重新创建 kubectl create -f nginx-ig.yml ○ 确认创建成功 配置本地机器 ○ 编辑本地主机host文件 C:\\Windows\\System32\\drivers\\etc\\hosts ○ 新增 192.168.1.21 k8s.zy.cn ○ 启动pod，发现返回404错误 ○ 编辑 ingress 修改配置项： kubectl -n zy-test edit ingress 1234567kind: Ingressmetadata: name: zy-nginx-ingress namespace: &quot;zy-test&quot; annotations: kubernetes.io/ingress.class: &quot;nginx&quot; nginx.ingress.kubernetes.io/rewrite-target: / # 新增配置项，进行重定向 访问路径 http://k8s.zy.cn/api 会自动重定向到 首页","categories":[{"name":"Kubernetes实战","slug":"Kubernetes实战","permalink":"http://jimoblivion.github.io/categories/Kubernetes%E5%AE%9E%E6%88%98/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://jimoblivion.github.io/tags/k8s/"},{"name":"Ingress","slug":"Ingress","permalink":"http://jimoblivion.github.io/tags/Ingress/"}]},{"title":"Kubernetes实战笔记（九）：Service服务发现","slug":"Kubernetes实战笔记（九）：Service服务发现","date":"2023-07-20T02:22:27.000Z","updated":"2023-07-20T12:06:30.821Z","comments":true,"path":"2023/07/20/Kubernetes实战笔记（九）：Service服务发现/","link":"","permalink":"http://jimoblivion.github.io/2023/07/20/Kubernetes%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B9%9D%EF%BC%89%EF%BC%9AService%E6%9C%8D%E5%8A%A1%E5%8F%91%E7%8E%B0/","excerpt":"","text":"容器之间通讯通过svc结合kube-proxy可以实现各个容器之间互通。 创建yml 123456789101112131415apiVersion: v1kind: Servicemetadata: name: nginx-svc labels: app: nginx namespace: &quot;zy-test&quot;spec: selector: app: nginx-deploy ports: - port: 80 targetPort: 80 name: web type: NodePort 创建svc 1kubectl create -f nginx-svc.yml 此时我们已经创建了label是nginx-deploy的pod 1kubectl -n zy-test get po -o wide svc创建好以后，会自动帮我们创建endpoints 运行一个新的容器 1kubectl run -it --image busybox:1.28.4 dns-test --restart=Never --rm /bin/sh 容器之间访问不通问题排查 访问被拒绝， 查看当前kube-proxy处于哪种模式 1kubectl get cm kube-proxy -n kube-system -o yaml | grep mode mode为空，默认使用iptable的模式 查看kube-proxy的日志 1docker ps | grep kube-proxy 1docker logs -f k8s_kube-proxy_kube-proxy-2k4tp_kube-system_78f24673-c269-48b9-9874-705cc5acf491_12 E0713 02:34:44.047737 1 proxier.go:1600] “can’t open port, skipping it” err&#x3D;”listen tcp4 :30082: bind: address already in use” port&#x3D;{Descripeb IP: IPFamily:4 Port:30082 Protocol:TCP} 推测可能是因为iptables未安装或启用导致，尝试使用ipvs。 详情见 Kube Proxy: Unable to open port when scheduling pods to kube master · Issue #107297 · kubernetes&#x2F;kubernetes (github.com) 安装ipvs 1yum install ipvsadm 修改mode为ipvs 1kubectl edit cm kube-proxy -n kube-system 12# 修改mode: &quot;ipvs&quot; 重启kube-proxy 1docker restart k8s_kube-proxy_kube-proxy-2k4tp_kube-system_78f24673-c269-48b9-9874-705cc5acf491_12 查看kube-proxy日志 1docker logs -f k8s_kube-proxy_kube-proxy-2k4tp_kube-system_78f24673-c269-48b9-9874-705cc5acf491_12 出现报错 E0713 02:51:35.448648 1 proxier.go:379] “Can’t set sysctl, kernel version doesn’t satisfy minimum version requirements” sysctl&#x3D;”net&#x2F;ipv4&#x2F;vs&#x2F;conn_reuse_mode” minimumKernelVersion&#x3D;”4.1” I0713 02:51:35.448815 1 proxier.go:438] “IPVS scheduler not specified, use rr by default” 根据描述，查看内核版本 1uname -r 此时需要对内核进行升级，来兼容ipvs 参考 CentOS 7 升级 Linux 内核-腾讯云开发者社区-腾讯云 (tencent.com) 安装完成后查看端口转发规则 1ipvsadm -Ln | grep 32023 -A2 32023转发规则下有两个 查看pod和ip能对应上 访问svc和对外的端口 1curl 192.168.1.20:32023 但是进入容器内部，执行 nslookup nginx-svc 不能获取到正确的域名和ip。推测当前容器所在的namespace和我访问的svc所在namespace不一致。 退出容器，在zy-test的命名空间内创建busybox 1kubectl -n zy-test run -it --image busybox:1.28.4 dns-test --restart=Never --rm /bin/sh 此时访问nginx-svc成功，说明容器内部之前互相通讯需要在同一命名空间下。 实现访问外部服务123实现方式： • 编写service配置文件时，不指定selector属性 • 自己创建endpoint yaml格式 123456789101112131415161718192021222324252627282930---apiVersion: v1kind: Servicemetadata: name: nginx-svc-ext labels: app: nginx namespace: &quot;zy-test&quot;spec: ports: - protocol: TCP port: 80 targetPort: 80 name: web type: ClusterIP---apiVersion: v1kind: Endpointsmetadata: labels: app: nginx name: nginx-svc-ext namespace: &quot;zy-test&quot;subsets:- addresses: - ip: xxx.xx.xx.xx # 外部ip ports: - name: web port: 80 protocol: TCP 代理外部域名 Service 定义将 zy-test 名称空间中的 nginx-svc-ext 服务映射到www.baidu.com 12345678910apiVersion: v1kind: Servicemetadata: labels: app: nginx name: nginx-svc-ext namespace: &quot;zy-test&quot;spec: type: ExternalName externalName: www.baidu.com 此处type的常用类型 ClusterIP: 只能在集群内部使用，不能配置类型的话默认就是ClusterIP ExternalName: 返回定义的CNAME别名，可以配置为域名 NodePort: 会在所有安装了kube-proxy的节点绑定一个端口，此端口可以代理至对应的Pod, 集群外部可以使用任意节点IP + NodePort的端口号访问集群中对应Pod的服务。 当类型设置为NodePort后，可以在Ports配置中增加NodePort配置指定端口，需要在下方端口范围内，如果不能指定会随机指定端口。 端口范围：30000~32767 端口范围在配置在 /usr/lib/systemd/system/kube-apiserver.service 文件中 LoadBalance: 使用云服务商提供负载均衡服务","categories":[{"name":"Kubernetes实战","slug":"Kubernetes实战","permalink":"http://jimoblivion.github.io/categories/Kubernetes%E5%AE%9E%E6%88%98/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://jimoblivion.github.io/tags/k8s/"},{"name":"Service","slug":"Service","permalink":"http://jimoblivion.github.io/tags/Service/"}]},{"title":"Kubernetes实战笔记（八）：HPA水平扩容","slug":"Kubernetes实战笔记（八）：HPA水平扩容","date":"2023-07-19T13:10:35.000Z","updated":"2023-07-20T03:27:10.578Z","comments":true,"path":"2023/07/19/Kubernetes实战笔记（八）：HPA水平扩容/","link":"","permalink":"http://jimoblivion.github.io/2023/07/19/Kubernetes%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0%EF%BC%88%E5%85%AB%EF%BC%89%EF%BC%9AHPA%E6%B0%B4%E5%B9%B3%E6%89%A9%E5%AE%B9/","excerpt":"","text":"Pod自动扩容 可以根据cpu使用率或自定义指标（metrics）自动对pod进行扩容&#x2F;缩容。 1234567• 控制管理器每隔30s( 可以通过-horizontal-pod-autosacaler-sync-period修改)查询metrics的资源使用情况• 支持三种metrics类型 ○ 预定义metrics(比如Pod的cpu)以利用率的方式计算 ○ 自定义的Pod metrics，以原始值（raw value）的方式计算 ○ 自定义的object metrics• 支持两种metics查询方式：Heapster和自定义的REST API• 支持多metrics查询方式 Horizontal Pod AutoScaler（HPA）自动扩容和缩容 123• 通过观察pod的cpu、内存使用情况或自定义metrics指标进行自动的扩容或者缩容pod的数量。• 通常用于Deployment， 不适用于无法扩容/缩容的对象，比如DaemonSet。• 控制管理器每隔30s查询一次metircs的资源使用情况 创建资源1vim nginx-deploy.yml 1234567891011121314151617181920212223242526272829303132apiVersion: apps/v1kind: Deploymentmetadata: name: nginx-deploy labels: app: nginx-deploy test: 1.1.0 namespace: &#x27;zy-test&#x27;spec: replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: app: nginx-deploy template: metadata: labels: app: nginx-deploy spec: terminationGracePeriodSeconds: 15 containers: - name: nginx image: nginx:1.7.9 imagePullPolicy: IfNotPresent resources: requests: cpu: 10m memory: 128Mi limits: cpu: 200m memory: 256Mi restartPolicy: Always cpu、内存指标的监控 设置指标 1kubectl -n zy-test autoscale deploy nginx-deploy --cpu-percent=20 --min=2 --max=5 1234说明：--cpu-percent指的是cpu的百分比达到就进行扩容--min扩容以后最小的实例数是2个--max扩容以后最大的实例数是5个 查看deployment的状态，发现已经帮我们扩容到2个replicas 1kubectl -n zy-test get deploy 监控资源的占用 1kubectl -n zy-test top pod nginx-deploy-57d84f45f8-mrtqk API不可用 安装组件 1wget https://github.com/kubernetes-sigs/metrics-server/releases/download/v0.6.2/components.yaml -O metrics-server-components.yaml 编辑文件，找到containers进行修改 1vim metrics-server-components.yaml 12345678910spec: containers: - args: - --cert-dir=/tmp - --secure-port=4443 - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname - --kubelet-use-node-status-port - --metric-resolution=15s - --kubelet-insecure-tls # 默认不使用https image: registry.cn-hangzhou.aliyuncs.com/google_containers/metrics-server:v0.6.2 # 使用国内镜像地址 安装工具 1kubectl apply -f metrics-server-components.yam 查看运行状态 检测资源的占用情况 1kubectl -n zy-test top pods 测试，让nginx的并发压力升高创建SVC 确认我们pod的ip端口 1kubectl -n zy-test get pods -o wide 新建yaml 1vim nginx-svc.yml 123456789101112131415apiVersion: v1kind: Servicemetadata: name: nginx-svc labels: app: nginx namespace: &quot;zy-test&quot;spec: selector: app: negix-deploy ports: - port: 80 targetPort: 80 name: web type: NodePort 创建svc 1kubectl create -f nginx-svc.yml 查看创建的svc 1kubectl -n zy-test get svc 使用一个死循环的脚本进行测试 执行脚本 1while true; do wget -q -O- http://10.244.249.52 &gt; /dev/null ; done 查看我们的hpa，已经超过了20% 1kubectl -n zy-test get hpa 查看资源使用情况 查看deploy， 已经帮我们进行了扩容 1kubectl -n zy-test get deploy 结束死循环 稍等再查看我们的HPA和top 1kubectl -n zy-test get hpa 1kubectl -n zy-test top po 资源降下来后要等一段时间后才会慢慢帮助我们缩容","categories":[{"name":"Kubernetes实战","slug":"Kubernetes实战","permalink":"http://jimoblivion.github.io/categories/Kubernetes%E5%AE%9E%E6%88%98/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://jimoblivion.github.io/tags/k8s/"},{"name":"HPA","slug":"HPA","permalink":"http://jimoblivion.github.io/tags/HPA/"}]},{"title":"Kubernetes实战笔记（七）：DaemonSet应用","slug":"Kubernetes实战笔记（七）：DaemonSet应用","date":"2023-07-19T12:35:18.000Z","updated":"2023-07-19T13:53:44.850Z","comments":true,"path":"2023/07/19/Kubernetes实战笔记（七）：DaemonSet应用/","link":"","permalink":"http://jimoblivion.github.io/2023/07/19/Kubernetes%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%83%EF%BC%89%EF%BC%9ADaemonSet%E5%BA%94%E7%94%A8/","excerpt":"","text":"说明DaemonSet为我们每一个匹配到的Node部署一个守护进程 创建 配置yaml文件，并且创建DaemonSet 1vim nginx-ds.yml 1234567891011121314151617181920212223242526272829303132333435apiVersion: apps/v1kind: DaemonSetmetadata: name: fluentd namespace: zy-testspec: revisionHistoryLimit: 10 selector: matchLabels: app: logging template: metadata: labels: app: logging id: fluentd name: fluentd spec: containers: - name: fluentd-es image: agilestacks/fluentd-elasticsearch:v1.3.0 env: - name: FLUENTD_ARGS value: -qq volumeMounts: - name: containers mountPath: /var/lib/docker/containers - name: varlog mountPath: /var/log volumes: - hostPath: path: /var/lib/docker/containers name: containers - hostPath: path: /var/log name: varlog 创建daemonset 1kubectl create -f nginx-ds.yml 查看创建的ds 指定node节点DaemonSet会忽略Node的unschedulable状态，有两种方式来指定pod只运行再指定的Node节点上： nodeSelector: 只调度到匹配label的Node上 nodeAffinity（节点亲和力）: 功能更丰富的Node选择器，比如支持集合操作 podAffinity（Pod亲和力）: 调度到满足条件 的pod所在的Node上 使用NodeSelector 查看节点的label标签 1kubectl get nodes --show-labels 给worker节点打标签 1kubectl label no k8snode1 type=microservices 确认标签已经打上 给daemonset配置中设置nodeSelector 1kubectl -n zy-test edit ds fluentd 12345678910111213141516spec: revisionHistoryLimit: 10 selector: matchLabels: app: logging template: metadata: creationTimestamp: null labels: app: logging id: fluentd name: fluentd spec: nodeSelector: type: microservices # 修改 containers: 保存成功后 \\ 1kubectl -n zy-test get po -l app=logging -o wide 使用label条件，查看节点信息 如果我们需要给指定节点部署daemonset，只需要给我们的节点打上标签即可 1kubectl label no $&#123;node&#125; type=microservices 更新策略DaemonSet默认的更新策略是滚动更新 12345updateStrategy: rollingUpdate: maxSurge: 0 maxUnavailable: 1 type: RollingUpdate 不建议使用RollingUpdate，建议使用OnDelete模式，这样避免频繁更新ds，需要更新的pod删除即可， 减少资源的占用 12updateStrategy: type: Ondelete","categories":[{"name":"Kubernetes实战","slug":"Kubernetes实战","permalink":"http://jimoblivion.github.io/categories/Kubernetes%E5%AE%9E%E6%88%98/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://jimoblivion.github.io/tags/k8s/"},{"name":"DaemonSet","slug":"DaemonSet","permalink":"http://jimoblivion.github.io/tags/DaemonSet/"}]},{"title":"Kubernetes实战笔记（六）：StatefulSet更新","slug":"Kubernetes实战笔记（六）：StatefulSet更新","date":"2023-07-19T10:22:23.000Z","updated":"2023-07-19T13:03:57.166Z","comments":true,"path":"2023/07/19/Kubernetes实战笔记（六）：StatefulSet更新/","link":"","permalink":"http://jimoblivion.github.io/2023/07/19/Kubernetes%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0%EF%BC%88%E5%85%AD%EF%BC%89%EF%BC%9AStatefulSet%E6%9B%B4%E6%96%B0/","excerpt":"","text":"rollingUpdate滚动更新 由于pod是有序的，在StatefulSet中更新是基于pod的顺序倒序更新。 灰度发布（金丝雀发布） 目的是将上线后产生问题的影响降低到最低 利用滚动更新中的partition属性，实现灰度发布的效果： 例如我们有5个pod，如果当前的partition设置为3，那么此时滚动更新时，只会更新那些序号&gt;&#x3D;3的pod。 可以控制partition的值，决定只更新其中一部分pod，确认没有问题后再逐渐增大更新pod的数量，最终实现全部pod更新。 编辑statefulset， 修改partition的数量为1 1kubectl -n zy-test edit sts web 12345##### 修改 updateStrategy: rollingUpdate: partition: 1 type: RollingUpdate 更新sts 1kubectl -n zy-test patch statefulset web --type=&#x27;json&#x27; -p=&#x27;[&#123;&quot;op&quot;:&quot;replace&quot;,&quot;path&quot;:&quot;/spec/template/spec/containers/0/image&quot;,&quot;value&quot;:&quot;nginx:1.7.1&quot;&#125;]&#x27; 确认， web-0的未更新 1kubectl -n zy-test describe po web-0 web-1镜像版本更新成功 编辑statefulset， 修改partition的数量为0（实现镜像的全部更新） 1kubectl -n zy-test edit sts web 12345##### updateStrategy: rollingUpdate: partition: 0 type: RollingUpdate 更新成功后，查看镜像版本已经更新成功 1kubectl -n zy-test describe po web-0 设置为当删除pod时更新 修改yaml 12updateStrategy:type: OnDelete 修改版本 1kubectl -n zy-test patch statefulset web --type=&#x27;json&#x27; -p=&#x27;[&#123;&quot;op&quot;:&quot;replace&quot;,&quot;path&quot;:&quot;/spec/template/spec/containers/0/image&quot;,&quot;value&quot;:&quot;nginx:1.9.1&quot;&#125;]&#x27; 此时查看pod详情，镜像版本未发生变化 1kubectl -n zy-test describe po web-0 删除pod 1kubectl -n zy-test delete po web-0 删除pod后会重启新的pod:web-0 对web-0的pod进行描述 1kubectl -n zy-test describe po web-0 级联删除定义删除statefulset时会同时删除pods。删除statefulset， 此时pod被删除，但是svc不会被删除 测试 删除sts 1kubectl -n zy-test delete sts web 删除svc 1kubectl -n zy-test delete svc nginx 非级联删除 执行非级联删除 1kubectl -n zy-test delete sts web --cascade=false sts删除成功，但是pod和svc未删除 删除pod和svc 12kubectl -n zy-test delete po web-0 web-1kubectl -n zy-test delete svc nginx 删除pvc statefulset被删除以后PVC还会被保留，数据不再使用的也需要删除 12kubectl -n zy-test get pvckubectl -n zy-test delete pvc www-web-0","categories":[{"name":"Kubernetes实战","slug":"Kubernetes实战","permalink":"http://jimoblivion.github.io/categories/Kubernetes%E5%AE%9E%E6%88%98/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://jimoblivion.github.io/tags/k8s/"},{"name":"StatefulSet","slug":"StatefulSet","permalink":"http://jimoblivion.github.io/tags/StatefulSet/"}]},{"title":"Kubernetes实战笔记（五）：StatefulSet有状态服务","slug":"Kubernetes实战笔记（五）：StatefulSet有状态服务","date":"2023-07-19T06:10:04.000Z","updated":"2023-07-19T10:57:28.799Z","comments":true,"path":"2023/07/19/Kubernetes实战笔记（五）：StatefulSet有状态服务/","link":"","permalink":"http://jimoblivion.github.io/2023/07/19/Kubernetes%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%94%EF%BC%89%EF%BC%9AStatefulSet%E6%9C%89%E7%8A%B6%E6%80%81%E6%9C%8D%E5%8A%A1/","excerpt":"","text":"说明 StatefulSet是专门针对有状态服务进行部署的一个控制器。 创建 创建yaml 1vim nginx-ss.yml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051---apiVersion: v1kind: Servicemetadata: name: nginx labels: app: nginx namespace: &quot;zy-test&quot;spec: ports: - port: 80 name: web clusterIP: None selector: app: nginx---apiVersion: apps/v1kind: StatefulSetmetadata: name: web namespace: &quot;zy-test&quot;spec: serviceName: &quot;nginx&quot; replicas: 2 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.7.9 ports: - containerPort: 80 name: web volumeMounts: - name: www mountPath: /usr/share/nginx/html volumeClaimTemplates: - metadata: name: www annotations: volume.alpha.kubernetes.io/storage-class: anything spec: accessModes: [ &quot;ReadWriteOnce&quot; ] resources: requests: storage: 1Gi 创建statefulset 1kubectl create -f nginx-ss.yml 查看pod 1kubectl -n zy-test get pod 查看pvc 1kubectl -n zy-test get pvc 查看sts 1kubectl -n zy-test get sts 查看私有卷 1kubectl -n zy-test get pvc 查看pvc的描述，此时statefulSet创建失败，原因是没有可用的持久卷 1kubectl -n zy-test describe pvc 查看sts 1kubectl -n zy-test get sts 所以此处将存储卷相关内容进行删除，并且重新创建sts StatefulSet中每个Pod的DNS格式为statefulSetName-(0..N-1).serviceName.namespace.svc.cluster.local serviceName为Headless Service的名字 0..N-1为Pod所在的序号，从0开始到N-1 statefulSetName为StatefulSet的名字 namespace为服务所在的namespace，Headless Service和StatefulSet必须在想用的namespace .cluster.local为Cluster Domain 1234567891011121314# 以下需要删除的内容volumeMounts: - name: www mountPath: /usr/share/nginx/htmlvolumeClaimTemplates:- metadata: name: www annotations: volume.alpha.kubernetes.io/storage-class: anything spec: accessModes: [ &quot;ReadWriteOnce&quot; ] resources: requests: storage: 1Gi 创建成功后，进入容器内部，解析dns并且使用web-0.nginx访问创建的容器 1kubectl run -it --image busybox:1.28.4 dns-test --restart=Never --rm /bin/sh 操作sts扩容和缩容 执行扩容 1kubectl -n zy-test scale statefulset web --replicas=3 执行缩容 1kubectl -n zy-test scale statefulset web --replicas=2 查看扩容和缩容的过程 1kubectl -n zy-test describe sts web sts更新 镜像更新 1kubectl -n zy-test patch statefulset web --type=&#x27;json&#x27; -p=&#x27;[&#123;&quot;op&quot;:&quot;replace&quot;,&quot;path&quot;:&quot;/spec/template/spec/containers/0/image&quot;,&quot;value&quot;:&quot;nginx:1.9.1&quot;&#125;]&#x27; 查看更新状态 1kubectl -n zy-test rollout status statefulset web 查看更新历史 1kubectl -n zy-test rollout history statefulset web 回退到指定的历史版本 1kubectl -n zy-test rollout history statefulset web --revision=2","categories":[{"name":"Kubernetes实战","slug":"Kubernetes实战","permalink":"http://jimoblivion.github.io/categories/Kubernetes%E5%AE%9E%E6%88%98/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://jimoblivion.github.io/tags/k8s/"},{"name":"StatefulSet","slug":"StatefulSet","permalink":"http://jimoblivion.github.io/tags/StatefulSet/"}]},{"title":"Kubernetes实战笔记（四）：资源调度Deployment","slug":"Kubernetes实战笔记（四）：资源调度Deployment","date":"2023-07-18T10:20:17.000Z","updated":"2023-07-19T03:59:28.880Z","comments":true,"path":"2023/07/18/Kubernetes实战笔记（四）：资源调度Deployment/","link":"","permalink":"http://jimoblivion.github.io/2023/07/18/Kubernetes%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0%EF%BC%88%E5%9B%9B%EF%BC%89%EF%BC%9A%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6Deployment/","excerpt":"","text":"说明Deployment是用来管理pod和Replicas pod部署、副本设定、滚动升级、回滚等功能。 创建编辑yml1vim nginx-deploy.yaml 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162apiVersion: apps/v1kind: Deploymentmetadata: annotations: deployment.kubernetes.io/revision: &quot;1&quot; creationTimestamp: &quot;2023-06-30T05:22:41Z&quot; generation: 1 labels: app: nginx-deploy name: nginx-deploy namespace: zy-test resourceVersion: &quot;18422&quot; uid: dac95cad-f455-4ca4-a8cd-569d8bbfdfdcspec: progressDeadlineSeconds: 600 replicas: 1 revisionHistoryLimit: 10 # 指定deployment保留多少revision,如果设置为0，则不允许deployment回退 selector: matchLabels: app: nginx-deploy strategy: rollingUpdate: maxSurge: 25% maxUnavailable: 25% type: RollingUpdate template: metadata: creationTimestamp: null labels: app: nginx-deploy spec: containers: - image: nginx:1.7.9 imagePullPolicy: IfNotPresent name: nginx resources: &#123;&#125; terminationMessagePath: /dev/termination-log terminationMessagePolicy: File dnsPolicy: ClusterFirst restartPolicy: Always schedulerName: default-scheduler securityContext: &#123;&#125; terminationGracePeriodSeconds: 30status: availableReplicas: 1 conditions: - lastTransitionTime: &quot;2023-06-30T05:22:43Z&quot; lastUpdateTime: &quot;2023-06-30T05:22:43Z&quot; message: Deployment has minimum availability. reason: MinimumReplicasAvailable status: &quot;True&quot; type: Available - lastTransitionTime: &quot;2023-06-30T05:22:41Z&quot; lastUpdateTime: &quot;2023-06-30T05:22:43Z&quot; message: ReplicaSet &quot;nginx-deploy-78d8bf4fd7&quot; has successfully progressed. reason: NewReplicaSetAvailable status: &quot;True&quot; type: Progressing observedGeneration: 1 readyReplicas: 1 replicas: 1 updatedReplicas: 1 创建deploy 执行创建脚本 1kubectl -n zy-test create deploy nginx-deploy.yaml 查看我们的po, rs，deploy并且显示labels 1kubectl -n zy-test get po,rs,deploy --show-labels 滚动更新只有修改了deployment配置文件中的template中的属性后，才会触发更新操作 修改镜像版本号 1kubectl -n zy-test set image deployment/nginx-deploy nginx=nginx:1.9.1 监控节点状态 1kubectl -n zy-test get deploy -w 查看deployment的描述, 版本号已经更新 1kubectl -n zy-test describe deployment/nginx-deploy 或者可以直接编辑保存deployment，效果一样 1kubectl -n zy-test edit deploy nginx-deploy 查看滚动更新状态 1kubectl -n zy-test rollout status deploy nginx-deploy 通过Id查看po，rs, deploy之间的关系 123kubectl -n zy-test get deploy --show-labelskubectl -n zy-test get rs --show-labelskubectl -n zy-test get po --show-labels 版本回滚当我们deployment不稳定，比如一直crash looping，此时我们可能需要回退之前的版本。默认系统中会保存前两次的rollout记录，可以修改revision history limit来更改revision的数量。 模拟不小心写错镜像版本（修改一个不存在的nginx的版本） 1kubectl -n zy-test set image deployment/nginx-deploy nginx=nginx:1.91 查看滚动升级的状态，发现 更新过程被卡住 1kubectl -n zy-test rollout status deploy nginx-deploy 查看我们rs和pod的信息，会发现ErrImagePull的信息 12kubectl -n zy-test get rs kubectl -n zy-test get pods 查看我们可以回退的revision的列表 1kubectl -n zy-test rollout history deployment/nginx-deploy 查看版本详情 1kubectl -n zy-test rollout history deployment/nginx-deploy --revision=2 确认需要回退后，进行版本回退 1kubectl -n zy-test rollout undo deployment/nginx-deploy 也可以指定版本进行回退 1kubectl -n zy-test rollout undo deploy nginx-deploy --to-revision=2 查看回退版本后deployment信息 1kubectl -n zy-test describe deploy nginx-deploy 12kubectl -n zy-test rollout undo deploy nginx-deploy --to-revision=1kubectl -n zy-test describe deploy nginx-deploy 扩容和缩容通过kube scale命令可以进行扩容和缩容，也可以使用kube edit编辑replicas实现扩容和缩容。扩容和缩容只是创建副本数量，没有更新pod template所以不会创建新的rs。 扩容 执行扩容操作 1kubectl -n zy-test scale --replicas=2 deploy nginx-deploy 查看扩容后状态 123kubectl -n zy-test get pokubectl -n zy-test get deploykubectl -n zy-test get rs 缩容 执行缩容操作 1kubectl -n zy-test scale --replicas=1 deploy nginx-deploy 查看缩容后状态 123kubectl -n zy-test get pokubectl -n zy-test get deploykubectl -n zy-test get rs 更新的暂停和恢复由于我们每次对pod template中的信息发生修改后，都会触发更新deployment操作。那么此时如果频发修改信息，就会产生多次更新，但是我们实际上只需要最后一次更新。这种情况下我们可以暂停deployment的rollout。 暂停更新 执行暂停脚本 1kubectl -n zy-test rollout pause deployment nginx-deploy 监听我们pod的状态 1kubectl -n zy-test get po -w 更新pod的中镜像的版本 1kubectl -n zy-test set image deploy nginx-deploy nginx=nginx:1.19.1 可以看到pod没有发生变化 我们再次修改一些属性 1kubectl -n zy-test set resources deploy nginx-deploy -c nginx --limits=cpu=200m,memory=128Mi --requests=cpu=100m,memory=64Mi 格式化输出deployment确认修改项，确认修改成功 1kubectl -n zy-test get deploy nginx-deploy -o yaml 从暂停中恢复 执行状态恢复脚本 1kubectl -n zy-test rollout resume deploy nginx-deploy 此时pod的状态发生了变更 查看更新历史和详情 1kubectl -n zy-test rollout history deploy nginx-deploy 1kubectl -n zy-test rollout history deploy nginx-deploy --revision=8","categories":[{"name":"Kubernetes实战","slug":"Kubernetes实战","permalink":"http://jimoblivion.github.io/categories/Kubernetes%E5%AE%9E%E6%88%98/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://jimoblivion.github.io/tags/k8s/"},{"name":"deployment","slug":"deployment","permalink":"http://jimoblivion.github.io/tags/deployment/"}]},{"title":"Kubernetes实战笔记（三）：标签label","slug":"Kubernetes实战笔记（三）：标签label","date":"2023-07-17T13:22:16.000Z","updated":"2023-07-19T02:45:44.599Z","comments":true,"path":"2023/07/17/Kubernetes实战笔记（三）：标签label/","link":"","permalink":"http://jimoblivion.github.io/2023/07/17/Kubernetes%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9A%E6%A0%87%E7%AD%BElabel/","excerpt":"","text":"添加临时标签 1kubectl -n zy-test label po nginx-pod app=hello 使用标签查看pod 1kubectl -n zy-test get po -A -l app=hello 使用标签查看pod，并且展示label属性 1kubectl -n zy-test get po -A -l app=hello --show-labels 查看pod上的label详情 1kubectl -n zy-test edit po nginx-pod 同一个标签匹配多个label值 1kubectl -n zy-test get po -l &#x27;app in (hello,test)&#x27; 多条件查询 1kubectl -n zy-test get po -l &#x27;app in (hello,test), type in (app)&#x27; 反向匹配 1kubectl -n zy-test get po -l &#x27;app!=test&#x27;","categories":[{"name":"Kubernetes实战","slug":"Kubernetes实战","permalink":"http://jimoblivion.github.io/categories/Kubernetes%E5%AE%9E%E6%88%98/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://jimoblivion.github.io/tags/k8s/"},{"name":"label","slug":"label","permalink":"http://jimoblivion.github.io/tags/label/"}]},{"title":"Kubernetes实战笔记（二）：pod的生命周期","slug":"Kubernetes实战笔记（二）：pod的生命周期","date":"2023-07-14T06:57:57.000Z","updated":"2023-07-18T15:27:27.426Z","comments":true,"path":"2023/07/14/Kubernetes实战笔记（二）：pod的生命周期/","link":"","permalink":"http://jimoblivion.github.io/2023/07/14/Kubernetes%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9Apod%E7%9A%84%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F/","excerpt":"","text":"Pod退出 Endpoint删除pod的ip地址 Pod变成Terminating状态 执行preStop的指令 PreStop应用 注册中心下线 数据清理 数据销毁 配置项描述terminationGracePeriodSeconds: Pod执行删除操作的时候还能维持一段时间，让pod去执行一些清理或者销毁的操作： 1234# 作用于pod中的所有容器terminationGracePeriodSeconds: 30containers: - xxx postStart: command和postStart可能同时执行，也就是容器创建完成后执行的动作，不能保证该操作一定在容器的command之前执行。所以我们一般不postStart, 而是使用初始化容器initC。 preStop: 在容器停止前执行的动作。 实践 创建pod 1vim nginx-pod.yml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748apiVersion: v1kind: Podmetadata:name: nginx-podlabels: type: app test: 1.0.0namespace: &#x27;zy-test&#x27;spec:containers:- name: nginx image: nginx:1.7.9 imagePullPolicy: IfNotPresent # 新增 begin lifecycle: postStart: # 生命周期之前 不能保证在容器的 command 之前，一般不用 exec: command: - sh - -c - &quot;echo &#x27;pre stop&#x27; &gt; /usr/share/nginx/html/prestop.html&quot; # &lt;&gt;需要转义 preStop: # 生命周期结束时执行 exec: command: - sh - -c - &quot;sleep 50; echo &#x27;sleep finished...&#x27; &gt;&gt; /usr/share/nginx/html/prestop.html&quot; # 新增 end command: - nginx - -g - &#x27;daemon off;&#x27; workingDir: /usr/share/nginx/html ports: - name: http containerPort: 80 protocol: TCP env: - name: JVM_OPTS value: &#x27;-Xms128m -Xmx128m&#x27; resources: requests: cpu: 100m memory: 128Mi limits: cpu: 200m memory: 256MirestartPolicy: OnFailure 1kubectl create -f nginx-pod.yml 查看pod，并且测试 postStart 是否生效 12kubectl -n zy-test get pod -o widecurl 10.244.249.10/prestop.html 删除pod，测试 preStop 是否生效 123# 查看pod的ipkubectl -n zy-test delete pod nginx-podcurl 10.244.249.10/prestop.html 新增 terminationGracePeriodSeconds 设置销毁后宽限时间 12spec:terminationGracePeriodSeconds: 15 # 容器销毁前给的宽限时间 12# 监听删除命令执行的时间time kubectl -n zy-test delete pod nginx-pod 我们可以在删除pod的同时持续监听pod的状态 1kubectl -n zy-test get pod -w 如需转载请说明出处","categories":[{"name":"Kubernetes实战","slug":"Kubernetes实战","permalink":"http://jimoblivion.github.io/categories/Kubernetes%E5%AE%9E%E6%88%98/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://jimoblivion.github.io/tags/k8s/"},{"name":"pod","slug":"pod","permalink":"http://jimoblivion.github.io/tags/pod/"}]},{"title":"Kubernetes实战笔记（一）：创建pod","slug":"Kubernetes实战笔记（一）：创建pod","date":"2023-07-12T08:25:20.000Z","updated":"2023-07-19T02:45:02.238Z","comments":true,"path":"2023/07/12/Kubernetes实战笔记（一）：创建pod/","link":"","permalink":"http://jimoblivion.github.io/2023/07/12/Kubernetes%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E5%88%9B%E5%BB%BApod/","excerpt":"","text":"概念pod是k8s的最小工作单元 创建pod 创建命名空间 1Kubectl create namespace zy-test 创建pod编写yaml 1vi nginx-pod.yml 123456789101112131415161718192021222324252627282930313233apiVersion: v1kind: Podmetadata: name: nginx-pod labels: type: app test: 1.0.0 namespace: &#x27;zy-test&#x27;spec: containers: - name: nginx image: nginx:1.7.9 imagePullPolicy: IfNotPresent command: - nginx - -g - &#x27;daemon off;&#x27; workingDir: /usr/share/nginx/html ports: - name: http containerPort: 80 protocol: TCP env: - name: JVM_OPTS value: &#x27;-Xms128m -Xmx128m&#x27; resources: requests: cpu: 100m memory: 128Mi limits: cpu: 200m memory: 256Mi restartPolicy: OnFailure 执行创建 1kubectl create -f nginx-pod.yml 此时pod一直处于pending状态 1Kubectl -n zy-test get pod | grep nginx-pod 查看pod的描述 1Kubectl -n zy-test describe pod nginx-pod Warning FailedScheduling 56s (x4 over 4m20s) default-scheduler 0&#x2F;1 nodes are available: 1 node(s) had taint {node-role.kubernetes.io&#x2F;master: }, that the pod didn’t tolerate. 此时默认master节点是不允许调度pod的，解决办法 删除污点或者新增从节点 1kubectl taint nodes --all node-role.kubernetes.io/master- 我们采用新增从节点的方式：clone新的节点作为从节点，并且修改静态ip地址 设置完成之后 设置hostname 1hostnamectl set-hostname k8sNode1 重置node节点 1kubeadm reset 加入master节点 123# 执行由master节点生成的token，在从节点执行kubeadm join 192.168.1.20:6443 --token abcdef.0123456789abcdef \\ --discovery-token-ca-cert-hash sha256:6d5a5dbc4cc7f67da955f1178e326cde55ed647613336a9fc990523bb6012b2e 给从节点打标签 1kubectl label node k8snode1 node-role.kubernetes.io/worker=worker 此时查看我们创建的nginx的pod已经运行 pod探针 我们可以使用pod探针可以获取当前容器的情况，探测方式有 exec: 在容器内部执行命令，如果返回值为0，则认为容器是健康的 tcpSocket: 通过tcp连接检测容器内端口是否开放，如果开放则证明该容器健康 httpGet: 发送HTTP请求到容器内部应用程序，如果接口返回状态码在200-400之间，则认为容器健康 startupProbe用于判断程序是否已经启动 修改nginx-pod.yml 12345678910111213141516...containers:- name: nginx image: nginx:1.7.9 imagePullPolicy: IfNotPresent# 新增 begin startupProbe: httpGet: path: /api/path port: 80 failureThreshold: 3 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 5 # 新增 end... 对pod进行描述 1kubectl describe pod -n zy-test nginx-pod 编辑pod修改 1kubectl edit pod -n zy-test nginx-pod 修改 123startupProbe:httpGet: path: /index.html 查看pod信息 1kubectl get pod -n zy-test nginx-pod -o wide 在节点上访问该nginx首页 跟踪pod日志 LivenessProbe用于探测容器中应用是否运行 编辑pod，新增如下配置 1234567891011121314containers:- name: nginx image: nginx:1.7.9 imagePullPolicy: IfNotPresent# 新增 begin livenessProbe: httpGet: path: /started.html #测试不存在的路径 port: 80 failureThreshold: 3 periodSeconds: 180 successThreshold: 1 timeoutSeconds: 5 # 新增 end 每隔一段时间pod会被杀掉重启 查看pod描述 拷贝文件start.html到容器中 12echo &quot;&lt;h1&gt;start up!&lt;/h1&gt;&quot; &gt; start.htmlkubectl cp ./start.html zy-test/nginx-pod:/usr/share/nginx/html/start.html 确认文件可以访问 查看pod的描述，恢复正常 ReadinessProbe用于探测容器内的程序是否健康 同上 如需转载请说明出处","categories":[{"name":"Kubernetes实战","slug":"Kubernetes实战","permalink":"http://jimoblivion.github.io/categories/Kubernetes%E5%AE%9E%E6%88%98/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://jimoblivion.github.io/tags/k8s/"},{"name":"pod","slug":"pod","permalink":"http://jimoblivion.github.io/tags/pod/"}]},{"title":"Docker和Kubernetes安装","slug":"Docker和Kubernetes安装","date":"2023-07-12T05:39:49.000Z","updated":"2023-07-18T15:27:43.844Z","comments":true,"path":"2023/07/12/Docker和Kubernetes安装/","link":"","permalink":"http://jimoblivion.github.io/2023/07/12/Docker%E5%92%8CKubernetes%E5%AE%89%E8%A3%85/","excerpt":"","text":"环境准备系统版本 系统 版本 CentOS release 7.8.2003 k8s和docker的版本选择Kubernetes版本选择1.23.1之前的，因为支持使用docker 软件 版本 Kubernetes（k8s） 1.23.0 Docker 20.10.0.3 安装docker安装 安装docker 1yum install -y docker-ce-20.10.0-3.el7 编辑文件 1vi /etc/docker/daemon.json 添加内容 1234567891011&#123;&quot;registry-mirrors&quot;: [ &quot;https://kfwkfulq.mirror.aliyuncs.com&quot;, &quot;https://2lqq34jg.mirror.aliyuncs.com&quot;, &quot;https://pee6w651.mirror.aliyuncs.com&quot;, &quot;https://registry.docker-cn.com&quot;, &quot;http://hub-mirror.c.163.com&quot;],&quot;dns&quot;: [&quot;8.8.8.8&quot;,&quot;8.8.4.4&quot;],&quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;]&#125; 重启docker服务 1systemctl restart docker 设置docker服务开机启动 1systemctl enable docker 安装docker-compose 下载docker-compose到随机目录 1https://github.com/docker/compose/releases/download/v2.18.1/docker-compose-linux-x86_64 给docker-compose执行权限 1chmod +x docker-compose-linux-x86_64 修改文件名 1mv docker-compose-linux-x86_64 docker-compose 确认环境变量目录 1echo $PATH 将docker-compose放入环境变量指定的目录，使docker-compose随处可以执行 1mv docker-compse /user/bin 确认配置成功 1docker-compose version 设置hostname, 并确认hostname修改成功 12hostnamectl set-hostname k8sMasterhostname host文件添加如下ip和hostname的映射关系 1echo &quot;127.0.0.1 $(hostname)&quot; &gt;&gt; /etc/hosts 安装k8s 组件安装 1yum install -y kubelet-1.23.0 kubeadm-1.23.0 kubectl-1.23.0 生成主节点配置 1kubeadm config print init-defaults &gt; kubeadm.yaml 修改配置文件 kubeadm.yaml 123456######### :11，修改 ######### localAPIEndpoint: advertiseAddress: &lt;这里换成master的地址&gt; ######### :30，修改 ######### imageRepository: registry.aliyuncs.com/google_containers ######### :33，新增podSubnet这一行 ######### networking: podSubnet: 10.244.0.0/16 主节点启动 123kubeadm config images pull --config kubeadm.yaml echo &quot;1&quot; &gt;/proc/sys/net/bridge/bridge-nf-call-iptables kubeadm init --config kubeadm.yaml 生成从节点加入主节点的token 123# 可以执行该脚本将从节点加入master节点kubeadm join 192.168.1.20:6443 --token abcdef.0123456789abcdef \\ --discovery-token-ca-cert-hash sha256:6d5a5dbc4cc7f67da955f1178e326cde55ed647613336a9fc990523bb6012b2e 如果使用非root用户，想要执行kubectl，运行以下命令 123mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/config 如果使root用户，则添加环境变量 1export KUBECONFIG=/etc/kubernetes/admin.conf 安装网络插件calico 下载yaml文件 1wget https://docs.projectcalico.org/v3.23/manifests/calico.yaml --no-check-certificate 确认镜像信息 1cat calico.yaml | grep -n image 删除docker.io 1sed -i &#x27;s#docker.io/##g&#x27; calico.yaml 安装calico 1kubectl apply -f calico.yaml 查看安装结果 1kubectl get pod -n kube-system | grep calico 确认节点信息 1kubectl get nodes 节点的添加和删除 使用主节点生成的token脚本在子节点执行即可。或者重新生成token 1kubeadm token create --print-join-command 关闭驱逐从节点 123456# 把slave1的资源驱赶到其他节点 kubectl drain k8s-slave1 --ignore-daemonsets # 标记slave1为不可调度 kubectl uncordon k8s-slave1 # 从集群删除slave1 kubectl delete node k8s-slave1 停止主节点 1kubectl drain node --ignore-daemonsets --delete-local-data --force 重启系统后遇到的问题 看节点信息发现报错The connection to the server 192.168.1.20:6443 was refused - did you specify the right host or port? 查看日志 1journalctl -fu kubelet 发现如下错误 failed to get cgroup stats for “&#x2F;system.slice&#x2F;docker.service”该问题只会发生在 CentOS 系统上，而引起上面的问题的原因是 kubelet 启动时，会执行节点资源统计，需要 systemd 中开启对应的选项 解决办法如下： 修改文件 1vi /usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf 1234[Service]CPUAccounting=true ## 添加 开启systemd CPU统计功能MemoryAccounting=true ## 添加 开启systemd Memory统计功能Environment=&quot;KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf&quot; 重启 Kubelet 服务，让 kubelet 重新加载配置。 12systemctl daemon-reloadsystemctl restart kubelet 重启完 kubelet 后等一段时间，再次观察 kubelet 日志信息，确认恢复正常 1journalctl -u kubelet -n 10 如需转载请说明出处","categories":[{"name":"Kubernetes安装","slug":"Kubernetes安装","permalink":"http://jimoblivion.github.io/categories/Kubernetes%E5%AE%89%E8%A3%85/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://jimoblivion.github.io/tags/k8s/"},{"name":"docker","slug":"docker","permalink":"http://jimoblivion.github.io/tags/docker/"}]},{"title":"赞助我","slug":"赞助我","date":"2023-07-12T05:39:49.000Z","updated":"2023-07-19T02:23:11.455Z","comments":true,"path":"2023/07/12/赞助我/","link":"","permalink":"http://jimoblivion.github.io/2023/07/12/%E8%B5%9E%E5%8A%A9%E6%88%91/","excerpt":"","text":"支持我","categories":[],"tags":[]}],"categories":[{"name":"Kubernetes实战","slug":"Kubernetes实战","permalink":"http://jimoblivion.github.io/categories/Kubernetes%E5%AE%9E%E6%88%98/"},{"name":"Kubernetes安装","slug":"Kubernetes安装","permalink":"http://jimoblivion.github.io/categories/Kubernetes%E5%AE%89%E8%A3%85/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://jimoblivion.github.io/tags/k8s/"},{"name":"initC","slug":"initC","permalink":"http://jimoblivion.github.io/tags/initC/"},{"name":"CronJob","slug":"CronJob","permalink":"http://jimoblivion.github.io/tags/CronJob/"},{"name":"StorageClass","slug":"StorageClass","permalink":"http://jimoblivion.github.io/tags/StorageClass/"},{"name":"pv","slug":"pv","permalink":"http://jimoblivion.github.io/tags/pv/"},{"name":"pvc","slug":"pvc","permalink":"http://jimoblivion.github.io/tags/pvc/"},{"name":"nfs","slug":"nfs","permalink":"http://jimoblivion.github.io/tags/nfs/"},{"name":"HostPath","slug":"HostPath","permalink":"http://jimoblivion.github.io/tags/HostPath/"},{"name":"EmptyDir","slug":"EmptyDir","permalink":"http://jimoblivion.github.io/tags/EmptyDir/"},{"name":"ConfigMap","slug":"ConfigMap","permalink":"http://jimoblivion.github.io/tags/ConfigMap/"},{"name":"Ingress","slug":"Ingress","permalink":"http://jimoblivion.github.io/tags/Ingress/"},{"name":"Service","slug":"Service","permalink":"http://jimoblivion.github.io/tags/Service/"},{"name":"HPA","slug":"HPA","permalink":"http://jimoblivion.github.io/tags/HPA/"},{"name":"DaemonSet","slug":"DaemonSet","permalink":"http://jimoblivion.github.io/tags/DaemonSet/"},{"name":"StatefulSet","slug":"StatefulSet","permalink":"http://jimoblivion.github.io/tags/StatefulSet/"},{"name":"deployment","slug":"deployment","permalink":"http://jimoblivion.github.io/tags/deployment/"},{"name":"label","slug":"label","permalink":"http://jimoblivion.github.io/tags/label/"},{"name":"pod","slug":"pod","permalink":"http://jimoblivion.github.io/tags/pod/"},{"name":"docker","slug":"docker","permalink":"http://jimoblivion.github.io/tags/docker/"}]}